# ğŸš€ Advanced RAG System

A production-ready **Retrieval-Augmented Generation (RAG)** platform featuring local LLM inference, hybrid retrieval, multi-agent orchestration, semantic caching, and full observability.

![Architecture](https://img.shields.io/badge/Architecture-Microservices-blue)
![Python](https://img.shields.io/badge/Python-3.11+-green)
![Docker](https://img.shields.io/badge/Docker-Compose-blue)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## âœ¨ Features

- **ğŸ¤– Local LLM Inference** - Run models locally with vLLM (no API costs for development)
- **ğŸŒ Cloud LLM Fallback** - Route complex queries to OpenRouter (Claude, GPT-4, etc.)
- **ğŸ” Hybrid Retrieval** - Dense + Sparse vector search with Qdrant
- **ğŸ“Š Full Observability** - Langfuse tracing with session & user tracking
- **ğŸ’¾ Semantic Caching** - Instant responses for similar queries
- **ğŸ“„ Multi-Format Ingestion** - PDF, DOCX, HTML, Markdown (+ OCR for images)
- **ğŸ¯ OpenAI-Compatible API** - Drop-in replacement for the OpenAI API

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              Open WebUI                                â”‚
â”‚                           (localhost:3000)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG Backend (FastAPI)                          â”‚
â”‚                           (localhost:5001)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Semantic   â”‚  â”‚   Query     â”‚  â”‚  Re-Ranker  â”‚  â”‚   Model     â”‚    â”‚
â”‚  â”‚   Cache     â”‚  â”‚  Rewriting  â”‚  â”‚  (Cross-Enc)â”‚  â”‚   Router    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                   â”‚                              â”‚
           â–¼                   â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Qdrant      â”‚  â”‚      vLLM       â”‚           â”‚    OpenRouter API   â”‚
â”‚  (Vector DB)    â”‚  â”‚  (Local LLM)    â”‚           â”‚   (Cloud Fallback)  â”‚
â”‚  localhost:6333 â”‚  â”‚  localhost:9999 â”‚           â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Observability Stack                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Langfuse   â”‚  â”‚ ClickHouse  â”‚  â”‚    MinIO    â”‚  â”‚    Redis    â”‚    â”‚
â”‚  â”‚  (UI:3001)  â”‚  â”‚   (OLAP)    â”‚  â”‚    (S3)     â”‚  â”‚   (Queue)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ³ Docker Services

| Container             | Image                           | Port      | Purpose                    |
| --------------------- | ------------------------------- | --------- | -------------------------- |
| `rag-open-webui`      | `ghcr.io/open-webui/open-webui` | 3000      | Chat UI (like ChatGPT)     |
| `rag-backend`         | Custom (Dockerfile)             | 5001      | FastAPI RAG orchestrator   |
| `rag-vllm`            | `vllm/vllm-openai`              | 9999      | Local LLM inference        |
| `rag-qdrant`          | `qdrant/qdrant`                 | 6333      | Vector database            |
| `rag-langfuse`        | `langfuse/langfuse:3`           | 3001      | Observability UI           |
| `rag-langfuse-worker` | `langfuse/langfuse-worker:3`    | 3030      | Trace processing           |
| `rag-clickhouse`      | `clickhouse/clickhouse-server`  | 18123     | Trace storage (OLAP)       |
| `rag-minio`           | `minio/minio`                   | 9000/9001 | S3-compatible blob storage |
| `rag-redis`           | `redis:7.2`                     | 6379      | Queue & cache              |
| `rag-langfuse-db`     | `postgres:16`                   | -         | Langfuse metadata DB       |

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker Desktop** (with GPU support for vLLM)
- **NVIDIA GPU** (Recommended, 8GB+ VRAM)
- **Git**

### 1. Clone & Configure

```bash
git clone https://github.com/yourusername/Advanced-RAG.git
cd Advanced-RAG

# Copy environment template
cp .env.example .env
# Edit .env with your API keys (OpenRouter, etc.)
```

### 2. Start All Services

```bash
docker compose up -d
```

### 3. Access the UI

- **Chat UI**: http://localhost:3000 (Open WebUI)
- **Langfuse Dashboard**: http://localhost:3001
- **API Docs**: http://localhost:5001/docs

### Default Langfuse Credentials

- Email: `admin@rag.local`
- Password: `ragadmin123`

---

## âš™ï¸ Environment Variables

| Variable             | Description                                | Default                      |
| -------------------- | ------------------------------------------ | ---------------------------- |
| `OPENROUTER_API_KEY` | API key for cloud LLM fallback             | Required for cloud models    |
| `LOCAL_MODEL_NAME`   | Model to run with vLLM                     | `Qwen/Qwen2.5-0.5B-Instruct` |
| `ENABLE_OCR`         | Enable OCR for image files (GPU intensive) | `false`                      |
| `LANGFUSE_DEBUG`     | Enable Langfuse debug logging              | `false`                      |
| `WEBUI_SECRET_KEY`   | Secret for Open WebUI sessions             | Set in compose               |

See `.env.example` for the full list.

---

## ğŸ“ Project Structure

```
Advanced-RAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app & endpoints
â”‚   â”œâ”€â”€ config.py               # Model & provider configuration
â”‚   â”œâ”€â”€ ingestion/              # Document processing pipeline
â”‚   â”‚   â”œâ”€â”€ router.py           # Ingestion orchestrator
â”‚   â”‚   â”œâ”€â”€ docling_parser.py   # PDF/DOCX parser
â”‚   â”‚   â”œâ”€â”€ deepseek_ocr.py     # OCR for images (optional)
â”‚   â”‚   â”œâ”€â”€ metadata.py         # LLM-based metadata extraction
â”‚   â”‚   â””â”€â”€ chunking.py         # Hierarchical chunking
â”‚   â”œâ”€â”€ retrieval/              # Search & retrieval
â”‚   â”‚   â”œâ”€â”€ engine.py           # Query rewriting, HyDE
â”‚   â”‚   â”œâ”€â”€ qdrant_client.py    # Vector DB operations
â”‚   â”‚   â””â”€â”€ reranker.py         # Cross-encoder reranking
â”‚   â”œâ”€â”€ generation/             # Response generation
â”‚   â”‚   â”œâ”€â”€ agents.py           # Multi-agent orchestration
â”‚   â”‚   â”œâ”€â”€ router.py           # Model routing (local/cloud)
â”‚   â”‚   â””â”€â”€ semantic_cache.py   # Query caching
â”‚   â””â”€â”€ observability/          # Monitoring
â”‚       â””â”€â”€ config.py           # Langfuse setup
â”œâ”€â”€ docker-compose.yml          # All services
â”œâ”€â”€ Dockerfile                  # RAG backend image
â”œâ”€â”€ pyproject.toml              # Python dependencies
â””â”€â”€ requirements.txt            # Pip dependencies
```

---

## ğŸ”„ How It Works

### Ingestion Pipeline (Upload a Document)

1. **File Detection** â†’ Route to Docling (PDF/DOCX) or OCR (images)
2. **Text Extraction** â†’ Preserve structure (tables, headers)
3. **Metadata Enrichment** â†’ LLM extracts department, date, summary
4. **Hierarchical Chunking** â†’ Parent (1024 tok) + Child (256 tok) chunks
5. **Vector Upsert** â†’ Dense + Sparse embeddings to Qdrant

### Query Pipeline (Ask a Question)

1. **Semantic Cache Check** â†’ Return cached answer if similarity > 0.95
2. **Query Rewriting** â†’ Expand ambiguous queries
3. **Hybrid Search** â†’ Dense (semantic) + Sparse (keyword) in Qdrant
4. **Re-ranking** â†’ Cross-encoder scores top 50 â†’ keep top 5
5. **Model Routing** â†’ Simple â†’ Local vLLM, Complex â†’ OpenRouter
6. **Response Generation** â†’ Stream answer with context
7. **Cache Update** â†’ Store Q&A for future queries

---

## ğŸ“Š Observability (Langfuse)

Access the Langfuse dashboard at http://localhost:3001

### Features

- **Traces** - Full execution path for each request
- **Sessions** - Group traces by conversation (chat thread)
- **Users** - Track usage per user
- **Costs** - Token usage and cost breakdown
- **Scores** - User feedback (thumbs up/down)

### Session Tracking

Open WebUI automatically sends session headers when `ENABLE_OPENWEBUI_USER_HEADERS=true`:

- `X-OpenWebUI-Chat-Id` â†’ Groups all messages in a conversation
- `X-OpenWebUI-User-Id` â†’ Links traces to users

---

## ğŸ› ï¸ Development

### Running Locally (without Docker)

```bash
# Install dependencies
pip install poetry
poetry install

# Start backend
poetry run uvicorn src.main:app --reload --port 8000
```

### Adding New Models

Edit `src/config.py` to add new models:

```python
ModelConfig(
    id="your-model-id",
    name="Display Name",
    provider=Provider.OPENROUTER,  # or Provider.VLLM
    context_window=8192,
)
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [LlamaIndex](https://www.llamaindex.ai/) - RAG framework
- [vLLM](https://github.com/vllm-project/vllm) - Fast LLM inference
- [Qdrant](https://qdrant.tech/) - Vector database
- [Langfuse](https://langfuse.com/) - LLM observability
- [Open WebUI](https://openwebui.com/) - Chat interface
- [Docling](https://github.com/DS4SD/docling) - Document parsing
